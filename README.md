# Eye Tracking Aviation – Gaze Analysis MVP

A desktop application that tracks a pilot's gaze via webcam, classifies it as
**inside** or **outside** the cockpit instrument panel (AOI), measures time in
each zone, and generates a structured debrief report after the session.

---

## Features

| Feature | Detail |
|---|---|
| Gaze tracking | MediaPipe FaceMesh iris landmarks |
| Calibration | 9-point polynomial ridge regression |
| AOI editor | Polygon drawn on cockpit image |
| Debounce | Configurable stable-ms hysteresis |
| EMA smoothing | Configurable alpha |
| Auto-pause | Warns when face lost > N seconds |
| Profiles | Per-participant `profiles/<name>/calibration.json` |
| Session markers | Press **M** → logged to `markers.csv` |
| Modes | Debug (overlay visible) + Test (clean) |
| Debrief | Stats, charts, state timeline, histogram |
| Replay | Scrub / play back gaze dot over cockpit image |
| Thresholds panel | Operator UI for live threshold tuning |
| File storage | All data as CSV/JSON, zero database |

---

## Quick Start

### 1 – Install dependencies

> **Windows note:** Use the **Anaconda** (or any non-system) Python to create
> the venv.  PySide6 ≥ 6.10 has a DLL incompatibility with some Windows builds;
> the pin `<6.10` in `pyproject.toml` handles this automatically.

```bash
# Use Anaconda Python (or any Python 3.11+ that is NOT the bare Windows store Python)
# On Windows with Anaconda installed:
C:\Users\<you>\anaconda3\python -m venv .venv

# Activate (Git Bash / MINGW64):
source .venv/Scripts/activate

# Activate (Command Prompt / PowerShell):
.venv\Scripts\activate

# Install all dependencies:
pip install -e ".[dev]"
```

### 2 – Add a cockpit image  *(optional but recommended)*

Drop a photograph of a cockpit into `assets/` named `cockpit.jpg`.
If none is found, a synthetic placeholder is used.
To generate the placeholder yourself:

```bash
python assets/generate_placeholder.py
```

### 3 – Run the application

**Option A – double-click (Windows, no terminal needed):**
```
run.bat
```

**Option B – from an activated venv terminal:**
```bash
# activate the venv first (see step 1), then:
python app/main.py
```

**Option C – explicit venv path (no activation needed):**
```bash
.venv/Scripts/python app/main.py      # Git Bash / MINGW64
.venv\Scripts\python app\main.py      # Command Prompt
```

---

## Calibration Workflow

1. Select / type a **profile name** on the home screen.
2. Click **Calibrate**.
3. **Step 1 – Gaze mapping**: 9 dots appear sequentially.
   Look directly at each dot and hold still.
   RMS error is shown after fitting; re-calibrate if > 0.06.
4. **Step 2 – AOI polygon**: click on the cockpit image to mark the instrument
   panel boundary (≥ 3 points; double-click to finish; right-click to undo).
5. Click **Save Calibration** – written to `profiles/<name>/calibration.json`.

---

## Session Modes

| Mode | Behaviour |
|---|---|
| **Debug** | Gaze dot + metrics panel overlaid on cockpit image |
| **Test** | No gaze overlay – participant sees only the cockpit image |

Press **M** at any time to insert a labelled marker.
Click **End Session** to stop recording and open the debrief screen.

---

## Output Files

Each session creates a folder under `runs/`:

```
runs/2026-02-24_14-32-10_test/
├── session_meta.json     ← mode, timestamps, camera info, calib hash
├── samples.csv           ← per-frame: timestamp, gaze_x/y_norm, confidence, state
├── events.csv            ← state transition log
├── markers.csv           ← manual markers (M key)
├── debrief.json          ← aggregated statistics
└── summary_export.csv    ← generated by Export button in debrief screen
```

---

## Settings (Thresholds Panel)

Open via **Settings** on the home screen or the ⚙ button.

| Setting | Default | Effect |
|---|---|---|
| Min Confidence | 0.30 | Below this → UNKNOWN state |
| EMA Alpha | 0.30 | Higher = more responsive, noisier |
| Stable ms | 200 | State must be stable this long before committing |
| Auto-pause | 3.0 s | Warn if face lost this long |
| Camera index | 0 | Webcam index (0 = default) |

Settings are persisted to `config.json`.

---

## Running Tests

```bash
pytest tests/ -v
```

Tests cover:
- `test_state_machine.py` – debounce / hysteresis logic
- `test_aoi.py` – polygon hit-test (cv2.pointPolygonTest)
- `test_metrics.py` – debrief statistics computation

---

## Project Structure

```
eye_tracking_aviation/
├── app/
│   ├── config.py           ← typed config + persistence
│   ├── controller.py       ← session lifecycle + vision worker thread
│   └── main.py             ← entry point
├── calibration/
│   ├── aoi_editor.py       ← polygon editor QWidget
│   └── gaze_calibration.py ← 9-point calibration QWidget
├── domain/
│   ├── models.py           ← dataclasses / enums
│   ├── state_machine.py    ← debounce logic
│   └── metrics.py          ← debrief stats
├── storage/
│   ├── session_writer.py   ← CSV/JSON file writer
│   └── calibration_store.py← load/save calibration profiles
├── ui/
│   ├── main_window.py      ← QMainWindow + stacked screens
│   ├── calibration_wizard.py
│   ├── session_screen.py
│   ├── debrief_screen.py
│   └── debug_overlay.py
├── vision/
│   ├── camera.py           ← threaded frame capture
│   ├── face_tracker.py     ← MediaPipe FaceMesh wrapper
│   ├── gaze_features.py    ← feature vector extraction
│   └── gaze_mapper.py      ← polynomial ridge regression + EMA
├── assets/
│   └── cockpit.jpg         ← place your cockpit image here
├── profiles/               ← calibration profiles (auto-created)
├── runs/                   ← session data directories (auto-created)
├── tests/
│   ├── test_state_machine.py
│   ├── test_aoi.py
│   └── test_metrics.py
├── pyproject.toml
└── README.md
```

---

## Troubleshooting

| Problem | Fix |
|---|---|
| `DLL load failed while importing QtWidgets` | You are using the wrong Python or PySide6 ≥ 6.10.  Recreate the venv with **Anaconda Python** (`anaconda3/python -m venv .venv`) and run `pip install -e ".[dev]"` — the `pyproject.toml` pin `PySide6<6.10` will install the working 6.9.x version automatically. |
| `RuntimeError: Cannot open camera` | Check `camera_index` in Settings; try 0, 1, 2 |
| Poor calibration (RMS > 0.06) | Improve lighting; ensure face is centred; redo calibration |
| Gaze jitter | Lower EMA alpha (more smoothing) in Settings |
| Face not detected | Check lighting; ensure both eyes visible; reduce Min Confidence |
| `AttributeError: module 'mediapipe' has no attribute 'solutions'` | MediaPipe 0.10.30+ removed the old API.  The app now uses the Tasks API automatically — delete any old venv and reinstall with `pip install -e ".[dev]"`. |
| `ModuleNotFoundError: No module named 'cv2'` | Run `pip install -e ".[dev]"` inside the activated venv |
| Model download fails on first run | Ensure internet access; the model (~10 MB) is saved to `assets/face_landmarker.task` automatically on first startup |

---

## Known Limitations & Next Steps

### Limitations (MVP)
- **Single-camera, single-monitor** only – gaze coordinates are normalised to the
  app window, not the physical screen; multi-monitor is unsupported.
- **Gaze model accuracy** depends heavily on consistent head position and
  good calibration (no head-mounted tracker).
- **No iris tracking fallback** – if MediaPipe loses tracking, state becomes UNKNOWN.
- **No per-frame accuracy metric** beyond EMA confidence.
- Window **must not be resized** between calibration and session (gaze coords
  would shift).

### Recommended Next Steps
1. **Better gaze model** – add head-pose (PnP from 3-D landmarks) as additional
   features or switch to a dedicated gaze library (OpenGaze, L2CS-Net).
2. **Multi-monitor support** – normalise to primary display coordinates.
3. **PDF debrief export** – use `reportlab` or `fpdf2`.
4. **Video recording** – capture annotated frames with `cv2.VideoWriter`.
5. **Real-time accuracy overlay** – show per-point calibration residuals.
6. **Fixation / saccade detection** – velocity-based I-VT classifier.
7. **Packaging** – `pyinstaller` one-file executable for lab deployment.
